{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e082744f",
   "metadata": {},
   "source": [
    "# Doc-GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c738d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c7fe247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca429dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b212ed",
   "metadata": {},
   "source": [
    "## a model that gives you answer about a specific text you are providing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff8c0121",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pipe_qa = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "262b5079",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa={\n",
    "    \"question\":\"What were the effect of the storm Debi?\",\n",
    "    \"context\": \"In Northern Ireland, the storm caused road closures and some disruption to the public transport network. NIE Networks said about 3,000 customers were without power, mainly around Craigavon, Newry and Downpatrick.\\\n",
    "On Tuesday afternoon, the Environment Agency had 15 flood warnings in place - meaning flooding is expected - and 102 lesser flood alerts.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3130858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0006679550861008465,\n",
       " 'start': 38,\n",
       " 'end': 103,\n",
       " 'answer': 'road closures and some disruption to the public transport network'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_qa(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a45d96e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'road closures and some disruption to the public transport network'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_qa_answer = pipe_qa(qa)[\"answer\"]\n",
    "pipe_qa_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234323b3",
   "metadata": {},
   "source": [
    "## a model for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730cdb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ba8065c46a498ba9f735ced4526c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c026a6fbd31042f0914480002833d251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/611M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2387b66932d54a6cafdec49fd7f3a98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b050c7d8ba9b45e99ed1e9b9f83df7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3f30b72428475d8c332eb0644f1758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe_summary = pipeline(\"summarization\", model=\"sshleifer/distilbart-xsum-12-6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7536ee73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Storm Debi has brought heavy rain and wind to parts of the UK.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_summary(\"A yellow warning for thunderstorms was in place for south-east England, parts of the south coast and London until 15:00 GMT on Tuesday. It follows stormy conditions across northern England and Scotland, with gusts of more than 70mph (112.7km/h) recorded in Wales and Northern Ireland. Debi is the fourth named storm of this winter so far. \\\n",
    "The rain and wind first hit Northern Ireland and the Republic of Ireland, then Wales, before moving eastwards and into the North Sea on Monday evening.\\\n",
    "In Northern Ireland, the storm caused road closures and some disruption to the public transport network. NIE Networks said about 3,000 customers were without power, mainly around Craigavon, Newry and Downpatrick.\\\n",
    "On Tuesday afternoon, the Environment Agency had 15 flood warnings in place - meaning flooding is expected - and 102 lesser flood alerts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e9cae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pipe_summary(\"A yellow warning for thunderstorms was in place for south-east England, parts of the south coast and London until 15:00 GMT on Tuesday. It follows stormy conditions across northern England and Scotland, with gusts of more than 70mph (112.7km/h) recorded in Wales and Northern Ireland. Debi is the fourth named storm of this winter so far. \\\n",
    "The rain and wind first hit Northern Ireland and the Republic of Ireland, then Wales, before moving eastwards and into the North Sea on Monday evening.\\\n",
    "In Northern Ireland, the storm caused road closures and some disruption to the public transport network. NIE Networks said about 3,000 customers were without power, mainly around Craigavon, Newry and Downpatrick.\\\n",
    "On Tuesday afternoon, the Environment Agency had 15 flood warnings in place - meaning flooding is expected - and 102 lesser flood alerts.\")[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c794f1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Storm Debi has brought heavy rain and wind to parts of the UK.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b50e83",
   "metadata": {},
   "source": [
    "## a model to give you answers about a specific document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0496cda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at impira/layoutlm-invoices were not used when initializing LayoutLMForQuestionAnswering: ['token_classifier_head.bias', 'token_classifier_head.weight']\n",
      "- This IS expected if you are initializing LayoutLMForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "pipe_mm = pipeline(\"document-question-answering\", model=\"impira/layoutlm-invoices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2069ac12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytesseract                       0.3.13\r\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f3f974",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9964991211891174, 'answer': '$2.89', 'start': 23, 'end': 23}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_mm(\"https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/receipt.webp\",\n",
    "    \"How much was the eggs cost?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6820504",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_mm = pipeline(\"document-question-answering\", model=\"impira/layoutlm-invoices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "350cd28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9964991211891174, 'answer': '$2.89', 'start': 23, 'end': 23}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_mm(\"/Users/carlobarbini/code/Karlobyo/doc_gpt_project/Doc_GPT/docs/receipt.png\", \n",
    "       \"How much was the eggs cost?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "325d8ec7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipe_mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/carlobarbini/code/Karlobyo/doc_gpt_project/Doc_GPT/docs/report.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow much was the eggs cost?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/transformers/pipelines/document_question_answering.py:271\u001b[0m, in \u001b[0;36mDocumentQuestionAnsweringPipeline.__call__\u001b[0;34m(self, image, question, word_boxes, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m image\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/transformers/pipelines/base.py:1249\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:33\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:186\u001b[0m, in \u001b[0;36mPipelineChunkIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# Try to return next item\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubiterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# When a preprocess iterator ends, we can start lookig at the next item\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# ChunkIterator will keep feeding until ALL elements of iterator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Another way to look at it, is we're basically flattening lists of lists\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# into a single list, but with generators\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/transformers/pipelines/document_question_answering.py:381\u001b[0m, in \u001b[0;36mDocumentQuestionAnsweringPipeline.preprocess\u001b[0;34m(self, input, padding, doc_stride, max_seq_len, word_boxes, lang, tesseract_config, timeout)\u001b[0m\n\u001b[1;32m    376\u001b[0m num_spans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# We put 0 on the tokens from the context and 1 everywhere else (question and special tokens)\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# This logic mirrors the logic in the question_answering pipeline\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m p_mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mspan_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_spans\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m span_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_spans):\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "pipe_mm(\"/Users/carlobarbini/code/Karlobyo/doc_gpt_project/Doc_GPT/docs/report.png\", \n",
    "       \"How much was the eggs cost?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed3dc7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "document-question-answering is already registered. Overwriting pipeline for task document-question-answering...\n",
      "/Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: You cannot use ``pop`` on a ModelOutput instance.\n"
     ]
    }
   ],
   "source": [
    "#from docquery import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipe_mm = pipeline('document-question-answering')\n",
    "\n",
    "# Load the image using PIL\n",
    "image_path = \"/Users/carlobarbini/code/Karlobyo/doc_gpt_project/Doc_GPT/docs/report.png\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Define your question\n",
    "question = \"How much was the eggs cost?\"\n",
    "\n",
    "# Call the pipeline\n",
    "try:\n",
    "    result = pipe_mm(image, question)\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7e820",
   "metadata": {},
   "source": [
    "This is the only conversational model I found from Hugging Face..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "310c25be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " They are a group of people who are in charge of the physical, emotional, social, and intellectual development of a person.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "\n",
    "# Load BlenderBot model and tokenizer\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Interact with BlenderBot\n",
    "inputs = tokenizer(\"What are menstruations?\", return_tensors=\"pt\")\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(tokenizer.decode(reply_ids[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cfa50b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26cef1ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docquery\n",
      "  Downloading docquery-0.0.7-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: torch>=1.0 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from docquery) (2.4.0)\n",
      "Collecting pdf2image (from docquery)\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pdfplumber (from docquery)\n",
      "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: Pillow in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from docquery) (10.4.0)\n",
      "Requirement already satisfied: pydantic in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from docquery) (2.8.2)\n",
      "Requirement already satisfied: pytesseract in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from docquery) (0.3.13)\n",
      "Requirement already satisfied: requests in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from docquery) (2.32.3)\n",
      "Collecting easyocr (from docquery)\n",
      "  Downloading easyocr-1.7.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: transformers>=4.23 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from docquery) (4.44.2)\n",
      "Requirement already satisfied: filelock in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from torch>=1.0->docquery) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from torch>=1.0->docquery) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from torch>=1.0->docquery) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from torch>=1.0->docquery) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from torch>=1.0->docquery) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from torch>=1.0->docquery) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from transformers>=4.23->docquery) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from transformers>=4.23->docquery) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from transformers>=4.23->docquery) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from transformers>=4.23->docquery) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from transformers>=4.23->docquery) (2024.7.24)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from transformers>=4.23->docquery) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from transformers>=4.23->docquery) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from transformers>=4.23->docquery) (4.66.5)\n",
      "Collecting torchvision>=0.5 (from easyocr->docquery)\n",
      "  Downloading torchvision-0.19.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.0 kB)\n",
      "Collecting opencv-python-headless (from easyocr->docquery)\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: scipy in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from easyocr->docquery) (1.14.1)\n",
      "Collecting scikit-image (from easyocr->docquery)\n",
      "  Downloading scikit_image-0.24.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Collecting python-bidi (from easyocr->docquery)\n",
      "  Downloading python_bidi-0.6.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (4.6 kB)\n",
      "Collecting Shapely (from easyocr->docquery)\n",
      "  Downloading shapely-2.0.6-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.0 kB)\n",
      "Collecting pyclipper (from easyocr->docquery)\n",
      "  Downloading pyclipper-1.3.0.post5-cp310-cp310-macosx_10_9_universal2.whl.metadata (9.0 kB)\n",
      "Collecting ninja (from easyocr->docquery)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-macosx_10_9_universal2.macosx_10_9_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl.metadata (5.3 kB)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber->docquery)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber->docquery)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from pdfminer.six==20231228->pdfplumber->docquery) (3.3.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20231228->pdfplumber->docquery)\n",
      "  Downloading cryptography-43.0.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from pydantic->docquery) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from pydantic->docquery) (2.20.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from requests->docquery) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from requests->docquery) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from requests->docquery) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from jinja2->torch>=1.0->docquery) (2.1.5)\n",
      "Collecting imageio>=2.33 (from scikit-image->easyocr->docquery)\n",
      "  Downloading imageio-2.35.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->easyocr->docquery)\n",
      "  Downloading tifffile-2024.8.24-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->easyocr->docquery)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from sympy->torch>=1.0->docquery) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber->docquery) (1.17.0)\n",
      "Requirement already satisfied: pycparser in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber->docquery) (2.22)\n",
      "Downloading docquery-0.0.7-py3-none-any.whl (35 kB)\n",
      "Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.19.0-cp310-cp310-macosx_11_0_arm64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-macosx_10_9_universal2.macosx_10_9_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl (270 kB)\n",
      "Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyclipper-1.3.0.post5-cp310-cp310-macosx_10_9_universal2.whl (277 kB)\n",
      "Downloading python_bidi-0.6.0-cp310-cp310-macosx_11_0_arm64.whl (250 kB)\n",
      "Downloading scikit_image-0.24.0-cp310-cp310-macosx_12_0_arm64.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shapely-2.0.6-cp310-cp310-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-43.0.0-cp39-abi3-macosx_10_9_universal2.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.35.1-py3-none-any.whl (315 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading tifffile-2024.8.24-py3-none-any.whl (225 kB)\n",
      "Installing collected packages: python-bidi, pyclipper, ninja, tifffile, Shapely, pypdfium2, pdf2image, opencv-python-headless, lazy-loader, imageio, scikit-image, cryptography, torchvision, pdfminer.six, pdfplumber, easyocr, docquery\n",
      "Successfully installed Shapely-2.0.6 cryptography-43.0.0 docquery-0.0.7 easyocr-1.7.1 imageio-2.35.1 lazy-loader-0.4 ninja-1.11.1.1 opencv-python-headless-4.10.0.84 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.4 pyclipper-1.3.0.post5 pypdfium2-4.30.0 python-bidi-0.6.0 scikit-image-0.24.0 tifffile-2024.8.24 torchvision-0.19.0\n"
     ]
    }
   ],
   "source": [
    "! pip install docquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb36d45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic==1.8.2\n",
      "  Downloading pydantic-1.8.2-py3-none-any.whl.metadata (103 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages (from pydantic==1.8.2) (4.12.2)\n",
      "Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.8.2\n",
      "    Uninstalling pydantic-2.8.2:\n",
      "      Successfully uninstalled pydantic-2.8.2\n",
      "Successfully installed pydantic-1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pydantic==1.8.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d840cb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "document-question-answering is already registered. Overwriting pipeline for task document-question-answering...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df00019b20e244999be56fc8cbbb1b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0e3ef9edd6426dab57f93ab1b6e32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/315 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbaef72f203440f84050a8163741cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b32a4b30d384094b4707a5a10fe0fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3142fd942d5947b2884b7d024996da30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3afa7d118f40ab8036051d817a95a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlobarbini/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b8b2b752de43279edf4ac38679d049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/511M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Exception",
     "evalue": "You cannot use ``pop`` on a ModelOutput instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m doc \u001b[38;5;241m=\u001b[39m document\u001b[38;5;241m.\u001b[39mload_document(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/carlobarbini/code/Karlobyo/doc_gpt_project/Doc_GPT_WEB/docs/receipt.webp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the invoice number?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the invoice total?\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(q, \u001b[43mp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/docquery/ext/pipeline_document_question_answering.py:232\u001b[0m, in \u001b[0;36mDocumentQuestionAnsweringPipeline.__call__\u001b[0;34m(self, image, question, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m     normalized_images \u001b[38;5;241m=\u001b[39m [(image, \u001b[38;5;28;01mNone\u001b[39;00m)]\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_images\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/transformers/pipelines/base.py:1249\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:294\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    293\u001b[0m         item \u001b[38;5;241m=\u001b[39m processed\n\u001b[0;32m--> 294\u001b[0m         is_last \u001b[38;5;241m=\u001b[39m \u001b[43mitem\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_last\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m         accumulator\u001b[38;5;241m.\u001b[39mappend(item)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/doc-gpt-env/lib/python3.10/site-packages/transformers/utils/generic.py:423\u001b[0m, in \u001b[0;36mModelOutput.pop\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot use ``pop`` on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: You cannot use ``pop`` on a ModelOutput instance."
     ]
    }
   ],
   "source": [
    "from docquery import document, pipeline\n",
    "p = pipeline('document-question-answering')\n",
    "doc = document.load_document(\"/Users/carlobarbini/code/Karlobyo/doc_gpt_project/Doc_GPT_WEB/docs/receipt.webp\")\n",
    "for q in [\"What is the invoice number?\", \"What is the invoice total?\"]:\n",
    "    print(q, p(question=q, **doc.context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6e22aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
